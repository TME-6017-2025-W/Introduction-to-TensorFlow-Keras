{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQZZ4wRg9M5S"
      },
      "source": [
        "# Training & evaluation with the built-in methods\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2019/03/01<br>\n",
        "**Last modified:** 2023/06/25<br>\n",
        "**Description:** Complete guide to training & evaluation with `fit()` and `evaluate()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zoIHlpS9M5W"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xa0vExHj9M5X"
      },
      "outputs": [],
      "source": [
        "# We import torch & TF so as to use torch Dataloaders & tf.data.Datasets.\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "# from keras import ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9okDBF29M5Y"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This guide covers training, evaluation, and prediction (inference) models\n",
        "when using built-in APIs for training & validation (such as `Model.fit()`,\n",
        "`Model.evaluate()` and `Model.predict()`).\n",
        "\n",
        "If you are interested in leveraging `fit()` while specifying your\n",
        "own training step function, see the guides on customizing what happens in `fit()`:\n",
        "\n",
        "- [Writing a custom train step with TensorFlow](/guides/custom_train_step_in_tensorflow/)\n",
        "- [Writing a custom train step with JAX](/guides/custom_train_step_in_jax/)\n",
        "- [Writing a custom train step with PyTorch](/guides/custom_train_step_in_torch/)\n",
        "\n",
        "If you are interested in writing your own training & evaluation loops from\n",
        "scratch, see the guides on writing training loops:\n",
        "\n",
        "- [Writing a training loop with TensorFlow](/guides/writing_a_custom_training_loop_in_tensorflow/)\n",
        "- [Writing a training loop with JAX](/guides/writing_a_custom_training_loop_in_jax/)\n",
        "- [Writing a training loop with PyTorch](/guides/writing_a_custom_training_loop_in_torch/)\n",
        "\n",
        "In general, whether you are using built-in loops or writing your own, model training &\n",
        "evaluation works strictly in the same way across every kind of Keras model --\n",
        "Sequential models, models built with the Functional API, and models written from\n",
        "scratch via model subclassing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbEJZIgZ9M5Y"
      },
      "source": [
        "## API overview: a first end-to-end example\n",
        "\n",
        "When passing data to the built-in training loops of a model, you should either use:\n",
        "\n",
        "- NumPy arrays (if your data is small and fits in memory)\n",
        "- Subclasses of `keras.utils.PyDataset`\n",
        "- `tf.data.Dataset` objects\n",
        "- PyTorch `DataLoader` instances\n",
        "\n",
        "In the next few paragraphs, we'll use the MNIST dataset as NumPy arrays, in\n",
        "order to demonstrate how to use optimizers, losses, and metrics. Afterwards, we'll\n",
        "take a close look at each of the other options.\n",
        "\n",
        "Let's consider the following model (here, we build in with the Functional API, but it\n",
        "could be a Sequential model or a subclassed model as well):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I9QFumst9M5Z"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9Q34Ntz9M5Z"
      },
      "source": [
        "Here's what the typical end-to-end workflow looks like, consisting of:\n",
        "\n",
        "- Training\n",
        "- Validation on a holdout set generated from the original training data\n",
        "- Evaluation on the test data\n",
        "\n",
        "We'll use MNIST data for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb-ZdyXZ9M5a",
        "outputId": "651a8d74-f22b-448f-885b-83079527d237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data (these are NumPy arrays)\n",
        "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
        "\n",
        "y_train = y_train.astype(\"float32\")\n",
        "y_test = y_test.astype(\"float32\")\n",
        "\n",
        "# Reserve 10,000 samples for validation\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiXllL6z9M5b"
      },
      "source": [
        "We specify the training configuration (optimizer, loss, metrics):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bpN9y1PF9M5b"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
        "    # Loss function to minimize\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    # List of metrics to monitor\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GADTaM_h9M5c"
      },
      "source": [
        "We call `fit()`, which will train the model by slicing the data into \"batches\" of size\n",
        "`batch_size`, and repeatedly iterating over the entire dataset for a given number of\n",
        "`epochs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jwrMmA99M5c",
        "outputId": "146b108e-a1f0-48c7-9062-6e0551b124d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fit model on training data\n",
            "Epoch 1/2\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3473 - sparse_categorical_accuracy: 0.9030 - val_loss: 0.1882 - val_sparse_categorical_accuracy: 0.9447\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.1627 - sparse_categorical_accuracy: 0.9510 - val_loss: 0.1441 - val_sparse_categorical_accuracy: 0.9601\n"
          ]
        }
      ],
      "source": [
        "print(\"Fit model on training data\")\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=2,\n",
        "    # We pass some validation for\n",
        "    # monitoring validation loss and metrics\n",
        "    # at the end of each epoch\n",
        "    validation_data=(x_val, y_val),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZMitPa-9M5c"
      },
      "source": [
        "The returned `history` object holds a record of the loss values and metric values\n",
        "during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fR4Jt_C9M5c",
        "outputId": "fbaf48d9-4662-42e1-ae7e-cace9ea61c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': [0.3400717079639435, 0.16072876751422882], 'sparse_categorical_accuracy': [0.9031800031661987, 0.9519000053405762], 'val_loss': [0.194541335105896, 0.13040010631084442], 'val_sparse_categorical_accuracy': [0.9422000050544739, 0.9638000130653381]}\n"
          ]
        }
      ],
      "source": [
        "print(history.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXXJJIS39M5d"
      },
      "source": [
        "We evaluate the model on the test data via `evaluate()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUWHa_Da9M5d",
        "outputId": "cc652702-0dfe-4a68-8609-3c1cdff974c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluate on test data\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 0.1410 - sparse_categorical_accuracy: 0.9582\n",
            "test loss, test acc: [0.1409795582294464, 0.9581999778747559]\n",
            "Generate predictions for 3 samples\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "predictions shape: (3, 10)\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "# Generate predictions (probabilities -- the output of the last layer)\n",
        "# on new data using `predict`\n",
        "print(\"Generate predictions for 3 samples\")\n",
        "predictions = model.predict(x_test[:3])\n",
        "print(\"predictions shape:\", predictions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28Gcjqml-1Vk",
        "outputId": "c0c6aab4-223c-40f7-a09b-f87a00626224"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.29677765e-05, 8.63194884e-07, 3.02126119e-03, 3.12428386e-03,\n",
              "        4.00560168e-07, 1.88762460e-05, 9.99802130e-10, 9.93506789e-01,\n",
              "        1.96923520e-05, 2.94737110e-04],\n",
              "       [1.15859557e-05, 1.10924866e-05, 9.98948634e-01, 8.98854807e-04,\n",
              "        1.31535838e-09, 1.25908373e-05, 7.77395599e-06, 1.84611426e-09,\n",
              "        1.09584827e-04, 8.52831861e-10],\n",
              "       [9.33492902e-06, 9.97568786e-01, 6.91366440e-04, 2.68980075e-04,\n",
              "        7.85170632e-05, 1.39705866e-04, 1.44345584e-04, 8.09750520e-04,\n",
              "        2.64381466e-04, 2.48236338e-05]], dtype=float32)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22kV4qsT9M5d"
      },
      "source": [
        "Now, let's review each piece of this workflow in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zfa3L8G9M5d"
      },
      "source": [
        "## The `compile()` method: specifying a loss, metrics, and an optimizer\n",
        "\n",
        "To train a model with `fit()`, you need to specify a loss function, an optimizer, and\n",
        "optionally, some metrics to monitor.\n",
        "\n",
        "You pass these to the model as arguments to the `compile()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w4CIozEh9M5d"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0lG7stx9M5d"
      },
      "source": [
        "The `metrics` argument should be a list -- your model can have any number of metrics.\n",
        "\n",
        "If your model has multiple outputs, you can specify different losses and metrics for\n",
        "each output, and you can modulate the contribution of each output to the total loss of\n",
        "the model. You will find more details about this in the **Passing data to multi-input,\n",
        "multi-output models** section.\n",
        "\n",
        "Note that if you're satisfied with the default settings, in many cases the optimizer,\n",
        "loss, and metrics can be specified via string identifiers as a shortcut:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FInXr5Ub9M5d"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUxWm7po9M5e"
      },
      "source": [
        "For later reuse, let's put our model definition and compile step in functions; we will\n",
        "call them several times across different examples in this guide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Eez2dhEn9M5e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_uncompiled_model():\n",
        "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_compiled_model():\n",
        "    model = get_uncompiled_model()\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"sparse_categorical_accuracy\"],\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9TkjZu99M5e"
      },
      "source": [
        "### Many built-in optimizers, losses, and metrics are available\n",
        "\n",
        "In general, you won't have to create your own losses, metrics, or optimizers\n",
        "from scratch, because what you need is likely to be already part of the Keras API:\n",
        "\n",
        "Optimizers:\n",
        "\n",
        "- `SGD()` (with or without momentum)\n",
        "- `RMSprop()`\n",
        "- `Adam()`\n",
        "- etc.\n",
        "\n",
        "Losses:\n",
        "\n",
        "- `MeanSquaredError()`\n",
        "- `KLDivergence()`\n",
        "- `CosineSimilarity()`\n",
        "- etc.\n",
        "\n",
        "Metrics:\n",
        "\n",
        "- `AUC()`\n",
        "- `Precision()`\n",
        "- `Recall()`\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQrEUxhj9M5e"
      },
      "source": [
        "### Custom losses\n",
        "\n",
        "If you need to create a custom loss, Keras provides three ways to do so.\n",
        "\n",
        "The first method involves creating a function that accepts inputs `y_true` and\n",
        "`y_pred`. The following example shows a loss function that computes the mean squared\n",
        "error between the real data and the predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbwO2Q0u9M5e",
        "outputId": "3804dc55-acfa-424a-bc71-93ec8e075534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 7s 6ms/step - loss: 0.0157\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ab2adf11840>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def custom_mean_squared_error(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
        "\n",
        "# Assuming get_uncompiled_model() is a function that returns a Keras model\n",
        "model = get_uncompiled_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)\n",
        "\n",
        "# We need to one-hot encode the labels to use MSE\n",
        "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
        "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PDrBOCv9M5f"
      },
      "source": [
        "If you need a loss function that takes in parameters beside `y_true` and `y_pred`, you\n",
        "can subclass the `keras.losses.Loss` class and implement the following two methods:\n",
        "\n",
        "- `__init__(self)`: accept parameters to pass during the call of your loss function\n",
        "- `call(self, y_true, y_pred)`: use the targets (y_true) and the model predictions\n",
        "(y_pred) to compute the model's loss\n",
        "\n",
        "Let's say you want to use mean squared error, but with an added term that\n",
        "will de-incentivize  prediction values far from 0.5 (we assume that the categorical\n",
        "targets are one-hot encoded and take values between 0 and 1). This\n",
        "creates an incentive for the model not to be too confident, which may help\n",
        "reduce overfitting (we won't know if it works until we try!).\n",
        "\n",
        "Here's how you would do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ2TD7tC9M5f"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomMSE(keras.losses.Loss):\n",
        "    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
        "        super().__init__(name=name)\n",
        "        self.regularization_factor = regularization_factor\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
        "        reg = tf.reduce_mean(tf.square(0.5 - y_pred), axis=-1)\n",
        "        return mse + reg * self.regularization_factor\n",
        "\n",
        "# Assuming get_uncompiled_model() is a function that returns a Keras model\n",
        "model = get_uncompiled_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF8PPWEO9M5f"
      },
      "source": [
        "### Custom metrics\n",
        "\n",
        "If you need a metric that isn't part of the API, you can easily create custom metrics\n",
        "by subclassing the `keras.metrics.Metric` class. You will need to implement 4\n",
        "methods:\n",
        "\n",
        "- `__init__(self)`, in which you will create state variables for your metric.\n",
        "- `update_state(self, y_true, y_pred, sample_weight=None)`, which uses the targets\n",
        "y_true and the model predictions y_pred to update the state variables.\n",
        "- `result(self)`, which uses the state variables to compute the final results.\n",
        "- `reset_state(self)`, which reinitializes the state of the metric.\n",
        "\n",
        "State update and results computation are kept separate (in `update_state()` and\n",
        "`result()`, respectively) because in some cases, the results computation might be very\n",
        "expensive and would only be done periodically.\n",
        "\n",
        "Here's a simple example showing how to implement a `CategoricalTruePositives` metric\n",
        "that counts how many samples were correctly classified as belonging to a given class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND9OJO-A9M5f",
        "outputId": "82e8a843-f102-4c4f-e918-549b4d6b3b92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.3340 - categorical_true_positives: 45309.0000\n",
            "Epoch 2/3\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1553 - categorical_true_positives: 47670.0000\n",
            "Epoch 3/3\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1123 - categorical_true_positives: 48317.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ab2ad2768c0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class CategoricalTruePositives(keras.metrics.Metric):\n",
        "    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.true_positives = self.add_weight(\n",
        "            name=\"ctp\", initializer=\"zeros\"\n",
        "        )\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), (-1, 1))\n",
        "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
        "        values = tf.cast(values, \"float32\")\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
        "            values = tf.multiply(values, sample_weight)\n",
        "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
        "\n",
        "    def result(self):\n",
        "        return self.true_positives\n",
        "\n",
        "    def reset_state(self):\n",
        "        # The state of the metric will be reset at the start of each epoch.\n",
        "        self.true_positives.assign(0.0)\n",
        "\n",
        "# Assuming get_uncompiled_model() is a function that returns a Keras model\n",
        "model = get_uncompiled_model()\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[CategoricalTruePositives()],\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr9kHx5D9M5f"
      },
      "source": [
        "### Handling losses and metrics that don't fit the standard signature\n",
        "\n",
        "The overwhelming majority of losses and metrics can be computed from `y_true` and\n",
        "`y_pred`, where `y_pred` is an output of your model -- but not all of them. For\n",
        "instance, a regularization loss may only require the activation of a layer (there are\n",
        "no targets in this case), and this activation may not be a model output.\n",
        "\n",
        "In such cases, you can call `self.add_loss(loss_value)` from inside the call method of\n",
        "a custom layer. Losses added in this way get added to the \"main\" loss during training\n",
        "(the one passed to `compile()`). Here's a simple example that adds activity\n",
        "regularization (note that activity regularization is built-in in all Keras layers --\n",
        "this layer is just for the sake of providing a concrete example):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oak5RfHN9M5g",
        "outputId": "f10bf3ac-2939-4f18-db98-25f4c77b913a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 3s 3ms/step - loss: 2.5080\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ab2ad012620>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "class ActivityRegularizationLayer(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        self.add_loss(tf.reduce_sum(inputs) * 0.1)\n",
        "        return inputs  # Pass-through layer.\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "\n",
        "# Insert activity regularization as a layer\n",
        "x = ActivityRegularizationLayer()(x)\n",
        "\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")\n",
        "\n",
        "# The displayed loss will be much higher than before\n",
        "# due to the regularization component.\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f89mnmrm9M5g"
      },
      "source": [
        "Note that when you pass losses via `add_loss()`, it becomes possible to call\n",
        "`compile()` without a loss function, since the model already has a loss to minimize.\n",
        "\n",
        "Consider the following `LogisticEndpoint` layer: it takes as inputs\n",
        "targets & logits, and it tracks a crossentropy loss via `add_loss()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PYiHc02Z9M5g"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LogisticEndpoint(keras.layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    def call(self, targets, logits, sample_weights=None):\n",
        "        # Compute the training-time loss value and add it\n",
        "        # to the layer using `self.add_loss()`.\n",
        "        loss = self.loss_fn(targets, logits, sample_weights)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # Return the inference-time prediction tensor (for `.predict()`).\n",
        "        return tf.nn.softmax(logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiLpXnYo9M5g"
      },
      "source": [
        "You can use it in a model with two inputs (input data & targets), compiled without a\n",
        "`loss` argument, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcrpYKfu9M5g",
        "outputId": "22d4ebb2-b63c-4330-84c2-56950da599de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 427ms/step - loss: 0.7340\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ab31b6de770>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
        "targets = keras.Input(shape=(10,), name=\"targets\")\n",
        "logits = keras.layers.Dense(10)(inputs)\n",
        "predictions = LogisticEndpoint(name=\"predictions\")(targets, logits)\n",
        "\n",
        "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
        "model.compile(optimizer=\"adam\")  # No loss argument!\n",
        "\n",
        "data = {\n",
        "    \"inputs\": np.random.random((3, 3)),\n",
        "    \"targets\": np.random.random((3, 10)),\n",
        "}\n",
        "model.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA9oBuyO9M5g"
      },
      "source": [
        "For more information about training multi-input models, see the section **Passing data\n",
        "to multi-input, multi-output models**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWlo72cz9M5h"
      },
      "source": [
        "### Automatically setting apart a validation holdout set\n",
        "\n",
        "In the first end-to-end example you saw, we used the `validation_data` argument to pass\n",
        "a tuple of NumPy arrays `(x_val, y_val)` to the model for evaluating a validation loss\n",
        "and validation metrics at the end of each epoch.\n",
        "\n",
        "Here's another option: the argument `validation_split` allows you to automatically\n",
        "reserve part of your training data for validation. The argument value represents the\n",
        "fraction of the data to be reserved for validation, so it should be set to a number\n",
        "higher than 0 and lower than 1. For instance, `validation_split=0.2` means \"use 20% of\n",
        "the data for validation\", and `validation_split=0.6` means \"use 60% of the data for\n",
        "validation\".\n",
        "\n",
        "The way the validation is computed is by taking the last x% samples of the arrays\n",
        "received by the `fit()` call, before any shuffling.\n",
        "\n",
        "Note that you can only use `validation_split` when training with NumPy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1etNptdC9M5h"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSGYLMi69M5v"
      },
      "source": [
        "For more information, see the\n",
        "[documentation for the `TensorBoard` callback](https://keras.io/api/callbacks/tensorboard/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
